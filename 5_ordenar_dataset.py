import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import shutil
import dask
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import shutil
from pathlib import Path # <-- AsegÃºrate de importar Path

# --- INICIO DE LA CORRECCIÃ“N ---
# Configurar Dask para que use un directorio temporal en el disco externo.
# Esto evita que se llene el disco principal del sistema (/tmp).
temp_dir = Path("data/dask-worker-space")
temp_dir.mkdir(parents=True, exist_ok=True)

dask.config.set({"temporary_directory": str(temp_dir)})
print(f"ðŸ”§ Dask usarÃ¡ el directorio temporal: {temp_dir.resolve()}")
# --- FIN DE LA CORRECCIÃ“N ---


# El resto de tu cÃ³digo continÃºa igual que antes...
#################################################################################
# ...
#################################################################################
#################################################################################

# El dataset ya tiene un orden excelente gracias a la particiÃ³n por aÃ±o y lÃ­nea, lo cual es fundamental para que las consultas sean rÃ¡pidas.
# Sin embargo, para una analÃ­tica aÃºn mÃ¡s eficiente, especialmente con series de tiempo,
# el siguiente paso recomendado es ordenar los datos por fecha y establecer esa columna como el Ã­ndice del dataset.


# Ordenar por Fecha (Set Index) ðŸ—“ï¸
# Dentro de cada archivo Parquet, las filas no estÃ¡n necesariamente en orden cronolÃ³gico.
# Ordenarlas por la columna fecha y establecerla como el "Ã­ndice" del dataset le da a Dask un superpoder.
#
# Consultas por Rango de Fechas UltrarrÃ¡pidas: Si pides "todos los viajes de la primera semana de mayo",
# Dask sabrÃ¡ exactamente en quÃ© parte de los archivos empezar y terminar de leer.
#
# AnÃ¡lisis de Series de Tiempo Eficiente: Operaciones como calcular promedios mÃ³viles (rolling averages)
# o re-muestrear los datos por semana o mes se vuelven mucho mÃ¡s rÃ¡pidas.


# historicos_parquet_ordenado
# serÃ¡ la versiÃ³n "dorada" de Ã©stos datos, perfectamente preparada para cualquier tipo de anÃ¡lisis que se quiera realizar.

#################################################################################
#################################################################################

# Rutas de entrada (el dataset simple) y de salida (el nuevo dataset ordenado)
PARQUET_IN = "data/dataset_subtes/historicos_parquet_simple"
PARQUET_OUT = "data/dataset_subtes/historicos_parquet_ordenado"

# Borra el destino anterior si existe
try:
    shutil.rmtree(PARQUET_OUT)
    print(f"ðŸ§¹ Carpeta de destino anterior eliminada: {PARQUET_OUT}")
except FileNotFoundError:
    pass

# Carga el dataset simplificado
print(f"Cargando dataset desde: {PARQUET_IN}")
ddf = dd.read_parquet(PARQUET_IN)

# --- El Paso Clave: Ordenar por fecha y establecerlo como Ã­ndice ---
print("\nOrdenando el dataset por fecha y estableciendo el Ã­ndice...")
# set_index realiza una clasificaciÃ³n masiva de los datos. Es una operaciÃ³n pesada.
ddf_ordenado = ddf.set_index('fecha')

# Guarda el nuevo dataset ordenado e indexado
print(f"\nGuardando dataset ordenado en: {PARQUET_OUT}")
with ProgressBar():
    # Al guardar un DataFrame con Ã­ndice, Dask gestiona la estructura de archivos
    # de forma optimizada para ese Ã­ndice.
    ddf_ordenado.to_parquet(
        PARQUET_OUT,
        engine="pyarrow",
        compression="zstd",
        overwrite=True
    )

print("\nâœ… Proceso de ordenamiento completado.")
print(f"Tu dataset final y optimizado para anÃ¡lisis estÃ¡ en: {PARQUET_OUT}")